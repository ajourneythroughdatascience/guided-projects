<article class="first">
  <div class="title">
    <h1>The State of Our World in 2023, Pt. 1</h1>
  </div>
</article>

---

[![made-with badge](https://img.shields.io/static/v1?label=Made%20with&message=Obsidian&color=7d5bed&logo=obsidian&labelColor=1a1a1a&style=flat)](https://obsidian.md/)

[![type](https://img.shields.io/static/v1?label=Type&message=blog&color=e60048&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAAAi0lEQVRIS+2WMQ7AIAhF/UNXrtP7rz2OYxeqTWxMTBUSxQVXfnzyQQKC8YExL7zAGCNbgIkIsIKVhBw4vbR7unR6Gp0LvwxXd2v+EvkdDpxWXpWlRTyi9/pABRyBJHEHSlxSadxSlV0SsVsqcUml2W/pynWxnsXNisHMRxrCl8qvH3ECnQDuOmy+0zwB4WNxmUKgwwAAAABJRU5ErkJggg==&labelColor=1a1a1a&style=flat)](https://pabloagn.com/blog/) [![category](https://img.shields.io/static/v1?label=Category&message=data-science&color=e60048&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAAB9UlEQVRIS6VWMU7DQBDkDAQEdrAoCISCAomCL1DxC95Azy9oeQS/oOIHVFAgREFoCHGCRSzZzEU+63LZ9W6CO/vudmZ2d9Zn1pTPaDSqut2usduHw+FpFEUv7t1fk8LNAkiPDWj3+ADuTPjNvXMxWwGzLCuqqtqwh5MkiY0xEwfOAfrEKFAWUBO4DZQDXgCEqjuouvbZUanUrocpngMMVUkKtKC+WhFQUudAUd8r1PkepJ/w7Tysn4uzkNJlascF9WOASAki6w0xrn19b3Gpps5y3kRfJADPZgr9gJSP0EgDHDiQ/Mp50PfxAmDtuQhsZmb/z0OVhwSkmGrSGp5bGRDp3EFaJ5JaiahdZ2vYNj/JkWVMgW7sgNw2yOW+99gacp7TeFE72OcUrgo4Ho93+/3+D5T9QmGHm0BNSnHgMI7jj9Ai2tElZGCK9S3S+GA4BcNNydBaIuEstu/iLJWCa+pLDm+Nz+xQAsBenucnRVG8asFq0s/Yf9YoVAI21wyn3N4I7M1A8ijWHwB42XrFqIO9YfMRlVqqyXC5ukED3nIEVJcoBXv1lmWa5gIpeeQioyTWVj1uXf0DpgKUZbmfpunXKnVnU9rWDKiTHRSDNkDu36iqIQK/Q+mxU8sBYniL/1EVoJ9Wqwo/5x6Cf9YKv6Em1XbNH5bGfSwvuRe1AAAAAElFTkSuQmCC&labelColor=1a1a1a&style=flat)](https://pabloagn.com/categories/data-science/) [![technologies](https://img.shields.io/static/v1?label=Technologies&message=Python&color=e60048&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA1klEQVR4nM2RMW7CUBBEnUikIQUIlBJxrrQgJG7ABRBnoUkaWhpoUgWJlgNYbvz/G1dUi1ayoy87rpOtVrszs6OdLPtXlef5UNJXjHHcCwohjMzsKZ3FGN+Bq/e+c0xHGfiWtEznkg6SNnW/dIxjs0YJ2AMnM3tJSFPgHkKY17gBcAQ+zOw5A3aSbsCkdW0NnNOZY2rstpcInJ3cS/SzwGdqtSzLmdusquqtIXWsehVF8QpcJK1qmxt/TMv6wjE/z0leP27i8Ag8inT/axxtAQ+9o/zn9QD3JOiyTjnQEQAAAABJRU5ErkJggg==&labelColor=1a1a1a&style=flat)](https://pabloagn.com/technologies/) [![website article](https://img.shields.io/static/v1?label=Website&message=Post%20Link&color=e60048&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+ElEQVR4nO2VOYgUURCGR/BAI4MN1EwjI89EMDYQvNBNNNlcE0VBUdlUUSMjj2BF2UDRePDAwGzNF2GNPIYd8Hjv/6YnEHSf/FIDPTJiu4nJFBTd1Kv6/nrVBd1q/S8DJiU9AmaBm5LOSjoATPwDY0LSQUnnzDArmJOjkqclvQceSHohaR6oJC1JeiPprqT9pZSVg5pSyirH4sw5S1EzbwZwP5jTIwWBdj1meEppZ6/XOyXpCdCX9Am4Fv45Yo+Bk1VV7ag3FNz2kKC7yznvHiX4u3U6nXU55xPAW7vfHfvLmNtmW8NaFux67k0Ea03esTfJJQTj23bHgiNtPNK6jZem3Wpg46Wp23hp2q0GNl6axksjaRGYkXRF0mnHq6ra2HSk/X5/k6RDks6YEazFPwnuBS5KuirptqTnkj4CJZ4zwNFSytqBoP/2wDHgXi33A/BM0i2zzDR7SBC4LGlPr9fb5huVUlYMus45b5E0FYJfgQS8C8/Al7jJVEpp86DODLPMNDs0up7xXBQZVKLLb8CCpIfA+ZzzvpTS+lLKGuAI8DT8cClltc+c49yoWQjGL140ao25oW8QXW1IKe3KOR8Hbkh66ZtI+i7plaG+iR244JjP3HDkXnetGWbVp9XYopHtHgvwWtIPu9+BSx7bssBNDdhqX07xT/Jbz1SBBDGHAAAAAElFTkSuQmCC&labelColor=1a1a1a&style=flat)](https://pabloagn.com/blog/programming-best-practices-writing-better-code/)

Aaaaaa

In this Blog Article, we'll provide the why and how of this series. We'll start by discussing motivations, expectations, and a brief overview of the state of the news today. We'll then define our document structure, provide an overview on measurement theory, & prepare our project using Python & R. Next, we'll provide a brief introduction to R, since it'll be our main language for this series (*an introduction to Python will also be provided when we get to the more advanced ML methods sections*).

Finally & with everything in place, we'll start by building our country hierarchy from public databases. This framework will conform the building blocks of all the analyses we'll be performing throughout this series.

We'll be using R scripts which can be found in the [Blog Article Repo](https://github.com/ajourneythroughdatascience/blog/tree/master/ides/beloved-vs-code-extensions-i-use-daily).

---

# Table of Contents
- [Legibility](#legibility)
	- [Authoring](#1-authoring)
	- [Comments](#2-comments)
- [Conclusions](#conclusions)
- [References](#references)
- [Copyright](#copyright)

---

# About this series
For some time now, I've been thinking about the flood of news and opinions we're exposed to daily. It's a lot. Everywhere we turn, there's talk about economic downturns, social upheavals, environmental crises, full-scale wars, or increased polarization. And if you're anything like me, this constant flow of information can leave you feeling anxious (*it even has an appointed term called "headline stress disorder", but more on this later*). You want to be involved but there's too much going on. 

Are things really as bad as they sound? Are they better or worst? Are we getting the whole story, or just fragments shaped by the loudest voices? Which sources of information are we consulting? Are they first-hand, second-hand, third-hand, or entirely made up? Are they government-based, or private-based? Are they leaning towards a left stand, a center stand or a right stand? Are our favorite news corporations owned by a multi-national conglomerate or by a non-profit organization?

Turns out, ingesting information is a lot like ingesting food, but even more critical really, because information has the capacity to shape our personality, the very things we stand for, and sometimes we don't even realize this. Just as we can lean towards a balanced diet, we can also choose to use reliable data and try to interpret it for ourselves.

Over the last five months, I've dug into the task of creating a data repository filled with gigabytes of reliable data from credible sources – a quest that's been daunting & bleak but also extremely enlightening & comforting. What I've found is a wealth of incredible organizations providing information that doesn't often make it into the daily news cycle, data that can help us see beyond the headlines and draw our own conclusions, and that is most often used by Economists or Statisticians, when it should be readily accessible to the general public (*yeah, some of these datasets hardly get past 9 downloads...per month*).

I'm not an Economist by any stretch. I'm simply a Data Scientist who's curious and a bit tired of all the misinformation and how it affects our capacity to focus resources on what really matters. This is why each week, throughout the month of December, I'll share my findings with you, examining different aspects of our world's current state – not to add to the noise, but to help us all make sense of it.

---

# What to expect
This will be a hands-on series. We'll not simply discuss results, but actually review the code in order to produce quality visualizations. 

We'll look at a lot of data from internationally-renowned sources like [The World Bank](https://www.worldbank.org/en/home), [UN](https://www.un.org/en/), [UNICEF](https://www.unicef.org/), [ILOSTAT](https://ilostat.ilo.org/), [World Health Organization](https://www.who.int/), and much more. I'll be learning right alongside you, breaking down complex economic concepts into something slightly more digestible. I'll always provide the raw data appropriately cited so you can always draw your own conclusions and use it however you want. I'll also include all the second hand resources that I used throughout this process and found superb at complementing the raw data. We'll discuss a lot of industry-standard metrics under the most relevant topics.

The goal? To equip ourselves with better tools for critical analysis. To move beyond anxiety-inducing headlines and understand the broader context – historical patterns, global comparisons, underlying causes and effects, the credibility and biases of different sources, and the intricate details that shape a story or issue. By the end of this series, we'll hopefully feel a bit more grounded and informed. Not just about the bad news, but about the paths to improvement and the many facets of our world that often go unnoticed.

For this series, we'll be mostly using [R](https://pabloagn.com/technologies/r/) & [RStudio](https://pabloagn.com/technologies/rstudio/), however, we'll also include additional technologies such as [Python](https://pabloagn.com/technologies/python/) for some Machine Learning methods, [JupyterLab](https://pabloagn.com/technologies/jupyter-lab/), [VS Code](https://pabloagn.com/technologies/vs-code/) & [PowerBI](https://pabloagn.com/technologies/powerbi/).

---

# The state of news today

## 1. Media Confidence
According to Reuters' Institute for the Study of Journalism [Digital News Report 2023](https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2023-06/Digital_News_Report_2023.pdf), only 10 out of 46 countries have a share of adult trust in news media as of February 2023 above 50%. This means that 78.2% of the surveyed countries have a trust share below 50%:

![[statistic_id308468_trustworthiness-of-news-media-worldwide-2023.png]]

###### Figure: [Share of adults who trust news media most of the time in selected countries worldwide as of February 2023, Reuters Institute for the Study of Journalism](https://www.statista.com/statistics/308468/importance-brand-journalist-creating-trust-news/)

Additionally, according to Reuters Institute, the gap between trust in news on platforms vs trust in news in general is significant for most of the platforms under study:


###### Figure: , Reproduced by Pablo Aguirre


According to Gallup, in the US alone, public trust in mass media has plummeted to 2016 record low numbers (*2016 included Donald Trump's presidential candidacy*). Moreover, we can see that the trend of the percentage of people not trusting mass media at all, has been steadily increasing since Gallup first started with this survey in 1972, as we can see from the chart below:

![[americans-trust-in-mass-media-1972-2023-.png]]

###### Figure: [Americans' Trust in Mass Media, 1972-2023, Gallup](https://news.gallup.com/poll/512861/media-confidence-matches-2016-record-low.aspx)

And this phenomenon is 

So, why are people still misinformed?

## 2. The QUIBBLE method
This article would be incomplete without first going over what I call the QUIBBLE method (*Questioning Underlying Information Before Believing, Learning, and Endorsing*). You see, when we're hit with news, the headline is often much more prevalent than the actual information contained in the article. This is because ["we live in a hypercompetitive world, and clickbait titles simply sell more news"](https://www.apa.org/monitor/2022/11/strain-media-overload).

Thus, when the data arrives, we already have a certain predisposition, and more specifically, we already felt something. This not only causes a potential bias, but is increasingly affecting people emotionally, as observed by Dr. Steven Stosny in his [_Washington Post_ opinion piece in 2017](https://www.washingtonpost.com/news/inspired-life/wp/2017/02/06/suffering-from-headline-stress-disorder-since-trumps-win-youre-definitely-not-alone/).

## 2. Breaking patterns
So how can we bypass this initial reaction, and focus on weather the piece is valid in the first place. Turns out there are some techniques we can (consciously) practice whenever we're exposed to news articles:
1. **Verifying the Primary Source**:
    - Ensure the primary source is credible and authoritative.
    - Check if the sources cited actually exist and are accessible.
    - Confirm that the data matches the information provided by the sources.
2. **Understanding Source Bias and Funding**:
    - Research the inclinations or biases of the source.
    - Identify the funding behind the research or data collection to uncover potential conflicts of interest.
3. **Analyzing the Metrics Used**:
    - Understand the metrics or indicators being used and how they are defined.
    - Assess the appropriateness of the metrics for the context or claim being made.
4. **Evaluating Data Collection Methods**:
    - Examine how the data was collected (e.g., surveys, experiments, administrative data).
    - Check for potential biases or errors in the data collection process.
5. **Inspecting Data Visualization Techniques**:
    - Look at the types of visualizations (graphs, charts, maps) used to present the data.
    - Assess whether the scales (linear, logarithmic) are appropriate and not misleading.
    - Ensure legends and labels are clear and accurate.
6. **Checking for Data Consistency and Timeliness**:
    - Ensure the data is consistent internally and with other credible sources.
    - Confirm that the data is current or relevant to the time period being discussed.
7. **Reviewing Statistical Analysis Methods**:
    - Understand the statistical methods used to analyze the data.
    - Check if the conclusions drawn are statistically valid and reasonably derived from the data.
8. **Considering Data Transparency and Reproducibility**:
    - Determine if the data and methodologies are transparent enough for independent verification.
    - Assess if the study or data analysis can be replicated or peer-reviewed.

## It happens to everyone, really
As a Data Scientist, I've gone over countless occasions where information is presented incorrectly (*I myself am guilty of this error*).


---
# Document structure
Since we'll be managing a lot of information & results, we'll keep an organized structure throughout the article. The framework will be as follows:
1. **Workspace setup using R & Python**: We'll be using both languages throughout this series. We'll focus on using R for the data transformations, statistic methods & visualizations. Python will be strongest when we start designing using Machine Learning models latter in the series. We'll create a project & environments for each case.
2. **A brief introduction to R**: Since much of what we'll do here will be related to designing & creating effective visualizations, we'll invest some time discussing how this works in R.
3. **Statistical Analysis & Metrics Visualization**: This will be the core of this series. We'll mostly focus on visualizing data from multiple sources, as well as perform statistical analyses if required. This section will be hierarchically divided as follows:
	1. **Segments**: Indicate the overall area of focus.
	2. **Subjects**: Indicate a more specific topic, and contain a group of metrics.
	3. **Metrics**: Measurements used to track and assess the status of a specific process. As we'll see soon, metrics can be classified into subgroups.

Below you will find a detailed breakdown of the Segments, Subjects & Metrics we'll tackle:

| Metric Code | Segment                            | Subject                                 | Source                                                                    |
| ----------- | ---------------------------------- | --------------------------------------- | ------------------------------------------------------------------------- |
| 01_01       | Macroeconomics                     | GDP                                     | The World Bank                                                            |
| 01_02       | Macroeconomics                     | GDP                                     | The World Bank                                                            |
| 01_03       | Macroeconomics                     | GDP                                     | The World Bank                                                            |
| 01_04       | Macroeconomics                     | GDP                                     | The World Bank                                                            |
| 01_05       | Macroeconomics                     | GDP                                     | The World Bank                                                            |
| 01_06       | Macroeconomics                     | Economic Metrics                        | The World Bank                                                            |
| 01_07       | Macroeconomics                     | Economic Metrics                        | International Monetary Fund (IMF)                                         |
| 01_08       | Macroeconomics                     | Economic Metrics                        | The World Bank                                                            |
| 01_09       | Macroeconomics                     | Economic Metrics                        | The Economist                                                             |
| 01_10       | Macroeconomics                     | Economic Metrics                        | United Nations Environment Programme (UNEP)                               |
| 01_11       | Macroeconomics                     | Economic Metrics                        | (blank)                                                                   |
| 01_12       | Macroeconomics                     | Economic Metrics                        | The World Bank                                                            |
| 01_13       | Macroeconomics                     | Economic Metrics                        | The World Bank                                                            |
| 01_14       | Macroeconomics                     | Unemployment & Underemployment          | The World Bank                                                            |
| 01_15       | Macroeconomics                     | Unemployment & Underemployment          | The World Bank                                                            |
| 01_16       | Macroeconomics                     | Unemployment & Underemployment          | International Labor Organization (ILOSTAT)                                |
| 01_17       | Macroeconomics                     | Debt & Deficit                          | International Monetary Fund (IMF)                                         |
| 01_18       | Macroeconomics                     | Debt & Deficit                          | Central Intelligence Agency (CIA)                                         |
| 01_19       | Macroeconomics                     | Income Inequality                       | The World Bank                                                            |
| 01_20       | Macroeconomics                     | Income Inequality                       | The World Bank                                                            |
| 01_21       | Macroeconomics                     | Income Inequality                       | The World Bank                                                            |
| 01_22       | Macroeconomics                     | Income Inequality                       | United Nations Development Programme (UNDP)                               |
| 01_23       | Macroeconomics                     | Poverty                                 | United Nations Development Programme (UNDP)                               |
| 01_24       | Macroeconomics                     | Poverty                                 | International Labor Organization (ILOSTAT)                                |
| 01_25       | Macroeconomics                     | Income Disparities & Wage Gap           | Credit Suisse                                                             |
| 01_26       | Macroeconomics                     | Income Disparities & Wage Gap           | World Economic Forum                                                      |
| 01_27       | Macroeconomics                     | Income Disparities & Wage Gap           | Organisation for Economic Co-operation and Development (OECD)             |
| 01_28       | Macroeconomics                     | Currency Value Against USD              | Organisation for Economic Co-operation and Development (OECD)             |
| 01_29       | Macroeconomics                     | Minimum Wage                            | International Labor Organization (ILOSTAT)                                |
| 01_30       | Macroeconomics                     | Labor Force Participation               | International Labor Organization (ILOSTAT)                                |
| 01_31       | Macroeconomics                     | Labor Force Participation               | International Labor Organization (ILOSTAT)                                |
| 01_32       | Macroeconomics                     | Household Dynamics                      | Organisation for Economic Co-operation and Development (OECD)             |
| 01_33       | Macroeconomics                     | Household Dynamics                      | Organisation for Economic Co-operation and Development (OECD)             |
| 01_34       | Macroeconomics                     | Household Dynamics                      | Organisation for Economic Co-operation and Development (OECD)             |
| 01_35       | Macroeconomics                     | Household Dynamics                      | Organisation for Economic Co-operation and Development (OECD)             |
| 01_36       | Macroeconomics                     | Household Dynamics                      | Organisation for Economic Co-operation and Development (OECD)             |
| 01_37       | Macroeconomics                     | Household Dynamics                      | Organisation for Economic Co-operation and Development (OECD)             |
| 01_38       | Macroeconomics                     | Household Dynamics                      | Organisation for Economic Co-operation and Development (OECD)             |
| 01_39       | Macroeconomics                     | Non-performing Loans and Solvency       | International Monetary Fund (IMF)                                         |
| 02_01       | Economic Indicators and Investment | Market and Consumer Indices             | Organisation for Economic Co-operation and Development (OECD)             |
| 02_02       | Economic Indicators and Investment | Market and Consumer Indices             | Organisation for Economic Co-operation and Development (OECD)             |
| 02_03       | Economic Indicators and Investment | Market and Consumer Indices             | The World Bank                                                            |
| 02_04       | Economic Indicators and Investment | Investment Trends                       | Organisation for Economic Co-operation and Development (OECD)             |
| 02_05       | Economic Indicators and Investment | Investment Trends                       | Organisation for Economic Co-operation and Development (OECD)             |
| 02_06       | Economic Indicators and Investment | Economic Metrics                        | United Nations World Tourism Organization                                 |
| 03_01       | Industrial and Trade               | Trade Patterns                          | International Monetary Fund (IMF)                                         |
| 03_02       | Industrial and Trade               | Industrial Environmental Impact         | The World Bank                                                            |
| 03_03       | Industrial and Trade               | Industrial Environmental Impact         | Our World in Data                                                         |
| 03_04       | Industrial and Trade               | Industrial Environmental Impact         | Our World in Data                                                         |
| 03_05       | Industrial and Trade               | Industrial Environmental Impact         | The World Bank                                                            |
| 03_06       | Industrial and Trade               | Consumption of Electricity              | The World Bank                                                            |
| 03_07       | Industrial and Trade               | Agriculture                             | The World Bank                                                            |
| 04_01       | Environmental Health               | Air Quality and Pollution               | IQAir                                                                     |
| 04_02       | Environmental Health               | Climate Change Indicators               | International Monetary Fund (IMF)                                         |
| 04_03       | Environmental Health               | Climate Change Indicators               | International Monetary Fund (IMF)                                         |
| 04_04       | Environmental Health               | Climate Change Indicators               | International Monetary Fund (IMF)                                         |
| 04_05       | Environmental Health               | Climate Change Indicators               | International Monetary Fund (IMF)                                         |
| 04_06       | Environmental Health               | Extreme Weather Conditions              | International Monetary Fund (IMF)                                         |
| 04_07       | Environmental Health               | Extreme Weather Conditions              | Germanwatch                                                               |
| 04_08       | Environmental Health               | Environmental Sustainability            | Yale University                                                           |
| 04_09       | Environmental Health               | Environmental Sustainability            | Dual Citizen Inc.                                                         |
| 04_10       | Environmental Health               | Environmental Sustainability            | Vision of Humanity                                                        |
| 04_11       | Environmental Health               | Percentage of Renewable Resources       | The World Bank                                                            |
| 05_01       | Political Sociology                | Corruption Overview                     | Transparency International                                                |
| 05_02       | Political Sociology                | Political Landscape                     | Economist Intelligence Unit                                               |
| 05_03       | Political Sociology                | Political Landscape                     | Freedom House                                                             |
| 05_04       | Political Sociology                | War and Armed Conflicts                 | Vision of Humanity                                                        |
| 05_05       | Political Sociology                | War and Armed Conflicts                 | Vision of Humanity                                                        |
| 05_06       | Political Sociology                | War and Armed Conflicts                 | Vision of Humanity                                                        |
| 05_07       | Political Sociology                | War and Armed Conflicts                 | Uppsala Universitet - Uppsala Conflict Data Program                       |
| 05_08       | Political Sociology                | War and Armed Conflicts                 | Uppsala Universitet - Uppsala Conflict Data Program                       |
| 05_09       | Political Sociology                | War and Armed Conflicts                 | Uppsala Universitet - Uppsala Conflict Data Program                       |
| 05_10       | Political Sociology                | War and Armed Conflicts                 | Uppsala Universitet - Uppsala Conflict Data Program                       |
| 05_11       | Political Sociology                | War and Armed Conflicts                 | Uppsala Universitet - Uppsala Conflict Data Program                       |
| 05_12       | Political Sociology                | War and Armed Conflicts                 | Uppsala Universitet - Uppsala Conflict Data Program                       |
| 05_13       | Political Sociology                | War and Armed Conflicts                 | Uppsala Universitet - Uppsala Conflict Data Program                       |
| 05_14       | Political Sociology                | War and Armed Conflicts                 | Uppsala Universitet - Uppsala Conflict Data Program                       |
| 05_15       | Political Sociology                | Political Inclinations                  | World Values Survey                                                       |
| 05_16       | Political Sociology                | Democracy and Governance                | Electoral Integrity Global Report                                         |
| 05_17       | Political Sociology                | Democracy and Governance                | Varieties of Democracy (V-Dem)                                            |
| 06_01       | Health and Well-Being              | Health                                  | The World Bank                                                            |
| 06_02       | Health and Well-Being              | Health                                  | The World Bank                                                            |
| 06_03       | Health and Well-Being              | Health                                  | World Health Organization (WHO), NCD Risk Factor Collaboration (NCD-RisC) |
| 06_04       | Health and Well-Being              | Health                                  | The World Bank                                                            |
| 06_05       | Health and Well-Being              | Health                                  | MIMI                                                                      |
| 06_06       | Health and Well-Being              | Health                                  | International Labor Organization (ILOSTAT)                                |
| 06_07       | Health and Well-Being              | Health                                  | International Labor Organization (ILOSTAT)                                |
| 06_08       | Health and Well-Being              | Global Burden of Disease                | The Institute for Health Metrics and Evaluation (IHME)                    |
| 06_09       | Health and Well-Being              | Global Burden of Disease                | The Institute for Health Metrics and Evaluation (IHME)                    |
| 06_10       | Health and Well-Being              | Global Burden of Disease                | The Institute for Health Metrics and Evaluation (IHME)                    |
| 06_11       | Health and Well-Being              | Global Burden of Disease                | The Institute for Health Metrics and Evaluation (IHME)                    |
| 06_12       | Health and Well-Being              | Well-Being                              | World Happiness Report                                                    |
| 06_13       | Health and Well-Being              | Mental Health                           | World Health Organization (WHO)                                           |
| 06_14       | Health and Well-Being              | Healthcare Access and Disparities       | World Health Organization (WHO)                                           |
| 06_15       | Health and Well-Being              | Healthcare Access and Disparities       | The World Bank                                                            |
| 06_16       | Health and Well-Being              | Substance Abuse                         | (blank)                                                                   |
| 07_01       | Educational Development            | Worldwide Education Trends              | The World Bank                                                            |
| 07_02       | Educational Development            | Education Disparities and Affordability | (blank)                                                                   |
| 07_03       | Educational Development            | Education Disparities and Affordability | (blank)                                                                   |
| 07_04       | Educational Development            | Maximum Degree of Studies               | Organisation for Economic Co-operation and Development (OECD)             |
| 07_05       | Educational Development            | Maximum Degree of Studies               | Organisation for Economic Co-operation and Development (OECD)             |
| 08_01       | Demographic Trends                 | Population Dynamics and Migration       | (blank)                                                                   |
| 08_02       | Demographic Trends                 | Population Dynamics and Migration       | (blank)                                                                   |
| 08_03       | Demographic Trends                 | Refugee Crisis                          | United Nations High Commissioner for Refugees (UNHCR)                     |
| 08_04       | Demographic Trends                 | Displaced Persons & Asylum Seekers      | (blank)                                                                   |
| 08_05       | Demographic Trends                 | Immigration and Integration             | Migrant Integration Policy Index (MIPEX)                                  |
| 08_06       | Demographic Trends                 | Natality Index                          | The World Bank                                                            |
| 08_07       | Demographic Trends                 | Life Conditions                         | The World Bank                                                            |
| 08_08       | Demographic Trends                 | Mortality Index                         | The World Bank                                                            |
| 08_09       | Demographic Trends                 | Total Population                        | The World Bank                                                            |
| 08_10       | Demographic Trends                 | Population Density                      | The World Bank                                                            |
| 08_11       | Demographic Trends                 | Religious Inclinations                  | (blank)                                                                   |
| 08_12       | Demographic Trends                 | Race                                    | (blank)                                                                   |
| 09_01       | Housing and Living Standards       | Housing Trends and Security             | (blank)                                                                   |
| 09_02       | Housing and Living Standards       | Housing Trends and Security             | (blank)                                                                   |
| 09_03       | Housing and Living Standards       | Housing Trends and Security             | (blank)                                                                   |
| 09_04       | Housing and Living Standards       | Basic Amenities Access                  | (blank)                                                                   |
| 09_05       | Housing and Living Standards       | Basic Amenities Access                  | (blank)                                                                   |
| 10_01       | Social and Humanitarian Issues     | Violence and Crimes                     | (blank)                                                                   |
| 10_02       | Social and Humanitarian Issues     | Violence and Crimes                     | (blank)                                                                   |
| 10_03       | Social and Humanitarian Issues     | Violence and Crimes                     | Small Arms Survey                                                         |
| 10_04       | Social and Humanitarian Issues     | Violence and Crimes                     | Small Arms Survey                                                         |
| 10_05       | Social and Humanitarian Issues     | Violence and Crimes                     | (blank)                                                                   |
| 10_06       | Social and Humanitarian Issues     | Violence and Crimes                     | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_07       | Social and Humanitarian Issues     | Violence and Crimes                     | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_08       | Social and Humanitarian Issues     | Violence and Crimes                     | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_09       | Social and Humanitarian Issues     | Violence and Crimes                     | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_10       | Social and Humanitarian Issues     | Drug-Related Metrics                    | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_11       | Social and Humanitarian Issues     | Drug-Related Metrics                    | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_12       | Social and Humanitarian Issues     | Drug-Related Metrics                    | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_13       | Social and Humanitarian Issues     | Drug-Related Metrics                    | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_14       | Social and Humanitarian Issues     | Drug-Related Metrics                    | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_15       | Social and Humanitarian Issues     | Drug-Related Metrics                    | United Nations Office on Drugs & Crime (UNODOC)                           |
| 10_16       | Social and Humanitarian Issues     | Child Welfare                           | United Nations International Children's Emergency Fund (UNICEF)           |
| 10_17       | Social and Humanitarian Issues     | Child Welfare                           | United Nations International Children's Emergency Fund (UNICEF)           |
| 10_18       | Social and Humanitarian Issues     | Child Welfare                           | United Nations International Children's Emergency Fund (UNICEF)           |
| 10_19       | Social and Humanitarian Issues     | Child Welfare                           | United Nations International Children's Emergency Fund (UNICEF)           |
| 10_20       | Social and Humanitarian Issues     | Nuclear and Security                    | Federation of American Scientists (FAS)                                   |
| 10_21       | Social and Humanitarian Issues     | Nuclear and Security                    | NTI Nuclear Security Index                                                |
| 10_22       | Social and Humanitarian Issues     | Social Injustice and Rights             | Othering & Belonging Institute                                            |
| 10_23       | Social and Humanitarian Issues     | Social Injustice and Rights             | Equaldex                                                                  |
| 10_24       | Social and Humanitarian Issues     | Social Injustice and Rights             | United Nations Development Programme (UNDP)                               |
| 10_25       | Social and Humanitarian Issues     | Social Injustice and Rights             | (blank)                                                                   |
| 10_26       | Social and Humanitarian Issues     | Social Injustice and Rights             | (blank)                                                                   |
| 10_27       | Social and Humanitarian Issues     | Social Injustice and Rights             | Reporters Without Borders (RSF)                                           |
| 10_28       | Social and Humanitarian Issues     | Social Injustice and Rights             | CATO Institute                                                            |
| 10_29       | Social and Humanitarian Issues     | Food Insecurity                         | Global Hunger Index                                                       |
| 10_30       | Social and Humanitarian Issues     | Food Insecurity                         | Food & Agriculture Organization of the United Nations                     |
| 10_31       | Social and Humanitarian Issues     | Food Insecurity                         | Economist Impact                                                          |
| 10_32       | Social and Humanitarian Issues     | Organized Crime                         | OC Index                                                                  |
| 10_33       | Social and Humanitarian Issues     | Travel Danger                           | US Department of State                                                    |
###### Table: [socioeconomic, political & environmental metrics](https://gist.github.com/pabloagn/5c1e0154659156fe8e15cb71705e2771)

---

# A word on metrics
Before we dive deeper into the data, we must understand what a metric is. A metric is a measure used to track and assess the status of a specific process. They can be classified into groups depending on:
- If our metric is independent, or depends on other metrics.
- If our metric measures a relationship between two numbers.
- If our metric shows a frequency of an occurrence, and not a single event.
- If our metric measures the intensity of a given phenomena.

We can try to group metrics under subgroups based on these attributes:
- **Indicator**: Indicators are specific, observable, and measurable attributes or changes that represent a concept of interest. An example would be the poverty rate, which indicates the proportion of the population living below the national poverty line.
- **Index**: An index is a composite metric that aggregates and standardizes multiple indicators to provide an overall score. A well-known example is the Human Development Index (HDI), which combines indicators of life expectancy, education, and income to rank countries into levels of human development.
- **Benchmark**: Benchmarks are standard points of reference against which things may be compared or assessed. A nice example would be voter turnout rates or the number of women in political office compared to international averages.
- **Ratio**: A ratio is a quantitative relationship between two numbers, showing how many times one value contains or is contained within the other. For example, the gender wage gap, which compares the median earnings of women and men.
- **Rate**: A rate is a special kind of ratio, showing the frequency of an occurrence in a defined population over a specific period of time. For instance, literacy rates or crime rates.
- **Scale**: Scales are used to measure the intensity or frequency of certain phenomena and often involve a range of values. The Air Quality Index (AQI), for example, measures the level of air pollution on a scale from 0 to 500.
- **Survey Results**: These are data collected directly from people, such as public opinion on government policies or satisfaction with public services.

Why is this important to understand? Well, because whenever we hear about some metric in the news, we're not always presented with how its calculated:

- Mistaking a composite index such as the Human Development Index (HDI) for a simple indicator like GDP per capita, drastically reduces the understanding of a country's development.
- Mentioning a literacy rate of 95%, without understanding the appropriate benchmark (*like the average literacy rate for its region or income group*) might completely bias the result towards a good outcome, when the real outcome when benchmarked correctly might be extremely low.
- Interpreting a moderate score on an air quality index (AQI) as "safe", without understanding the specific pollutants measured and their health impacts, can lead to underestimating health risks.
- Comparing GDPs between countries without [adjusting for Purchase Power Parity](https://ourworldindata.org/what-are-ppps), or simply using [Nominal GDP vs Real GDP](https://www.khanacademy.org/economics-finance-domain/ap-macroeconomics/economic-iondicators-and-the-business-cycle/real-vs-nominal-gdp/a/lesson-summary-real-vs-nominal-gdp) without acknowledging that the first one includes an inflationary component.
- Taking survey results at face value without considering potential biases in the survey design or the representativeness of the sample can lead to incorrect conclusions about public opinion. This is extremely common and often happens around political contexts, specifically at political polling (*[this Medium article](https://medium.com/@vanacorec/selection-bias-in-political-polling-9fd667e8e7f7) nicely explains this phenomenon*)

The bottom line is that statistics is a very powerful tool that can be used to gather insights, but also mislead people, purposefully or by accident. Knowing how metrics are designed & built is already an extremely valuable tool we can use [anytime Colgate decides to bring out a new advert on how good their product is](https://www.reuters.com/article/uk-britain-colgate-idUKL1654835620070117/).

Of course, knowing our metrics and their components & classification is just the first step in 

---

# Preparing our workspace
Now that the basic theory is all set, we can start by setting up our workspace. This will include:
- Setting up a folder structure.
- Setting up an R project & environment.
- Setting up a Python environment.
- installing required packages for both parties.

## Folder structure set up
Before we create our virtual environments, we'll set up our folder structure. Here's what we'll need:
- A `data` folder: Will contain all our input files.
	- With a subfolder for raw data
	- With a subfolder for processed data
- An `outputs` folder: Will contain all our results.
	- With a figures subfolder
	- With a results subfolder
- A `src` folder: Will contain our source code.
	- With a Python folder
	- With an R folder
- A `.gitignore` file: Will help us remove tracking of specific folders from our repository.

Hence, the tree structure will look something as such:

```
Project Root
│
├── src
│   ├── R
│   │   └── scripts.R
│   │
│   ├── Python
│   │   ├── scripts.py
│   │   └── venv (Python virtual environment)
│   │
│   └── requirements.txt (Python package requirements)
│
├── data
│   ├── raw (Raw data files)
│   └── processed (Processed data files)
│
├── output
│   ├── figures (Generated figures, charts)
│   └── results (Results in CSV, Excel, etc.)
│
├── .Rproj (R Project file)
│
└── .gitignore (To exclude files/folders from Git)
```

## Environment set up
Once we have clarity on our project's directory structure, we can set up our environments. We'll start with R, and then move on with Python.

### R
Throughout this segment we'll mostly be using RStudio. A detailed step-by-step tutorial can be found [here](https://rstudio-education.github.io/hopr/starting.html).

We'll set up two different components in R:
- **An R Project**: Will help us set a working directory associated with our project. It includes an `.Rproj` file which, when opened, sets the working directory to the location of this file and restores the session state. This will be useful for accessing inputs using relative paths.
- **An R Environment**: Will help us manage R dependencies, just as with Python.
- **Multiple R Scripts**: Will be the core of our project. They will contain all the code we'll use throughout this segment.

#### The R Project
We'll start by creating our project. For this, we'll use the Project Root directory we selected previously:
1. **Open RStudio**.
2. **Go to `File` > `New Project`**.
3. Choose **`Existing Directory`**.
4. Define the **location** chosen as the Project Root directory.
5. Click **`Create Project`**.

Now, depending on our Project Root directory name, we should have a new file with the `.Rproj` extension. This file works as a configuration file, and also sets the context for our project. 

#### The R Environment
We'll now proceed to create an R Environment specifically tailored for this project. From within RStudio we'll head to the R console and execute the following:

```R
install.packages("renv")
```

This will install the `renv` package if we don't have it already. We should get an output similar to the following:

```
package ‘renv’ successfully unpacked and MD5 sums checked
```

Once installed, we can activate our environment using the console:

```R
renv::init()
```

If everything went well, we should end up with the following new files & folders under our Project Root directory:
- `.Rprofile`: This is a script that runs automatically whenever you start an R session in a specific project or directory. It's used to set project-specific options, like library paths, default CRAN mirrors, or any R code you want to run at the start of each session.
- `renv.lock`: This file is created by the `renv` package. It's a lockfile that records the exact versions of R packages used in your project.
- `renv`: This is the folder containing our new environment, similar to what happens with Python `venv` or `virtualenv`.

#### Installing packages
For this series we'll need several R packages:
- Reading & writing
	- [`readr`](https://readr.tidyverse.org/): Provides the capability to read from comma-separated value (`CSV`) and tab-separated value (`TSV`) files. We'll use this for reading mainly from `csv` files.
	- [`readxl`](https://readxl.tidyverse.org/): Provides the capability to read Excel files, specifically tailored for well-known datasets (*faster, uses more RAM, only analyzes the first 1000 rows to determine the column types*). We'll use this for from Excel files.
	- [`openxlsx`](https://cran.r-project.org/web/packages/openxlsx/index.html): Provides the capability to read Excel files, specifically for big files with unknown values (*does not have the first-1000-row-reading limitation*). We'll use this for reading from Excel files.
	- [`writexl`](https://cran.r-project.org/web/packages/writexl/index.html): Provides a zero-dependency data frame to `xlsx` exporter based on `libxlsxwriter`. Fast and no Java or Excel required. We'll use this for our results exporting.
	- [`arrow`](https://cran.r-project.org/web/packages/arrow/index.html): Provides an interface to the 'Arrow C++' library. We'll use this for reading & writing Parquet files, which we'll explore once we get to the Machine Learning part of this series.
- Data manipulation
	- [`dplyr`](https://dplyr.tidyverse.org/): Data manipulation & complex transformations.
	- [`tidyr`](https://tidyr.tidyverse.org/): Changing the shape (*pivoting*) and hierarchy (*nesting and 'unnesting'*) of a dataset, by sticking to the tidy data framework: Each column is a variable, each row is an observation, and each cell contains a single value.
	- [`data.table`](https://cran.r-project.org/web/packages/data.table/index.html): Provides fast aggregation of large data, fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, and friendly and fast character-separated-value read/write. In short, a faster `data.frame` implementation.
	- [`stringr`](https://stringr.tidyverse.org/): Provides a cohesive set of functions designed to make working with strings as easy as possible.
	- [`lubridate`](https://lubridate.tidyverse.org/): Provides multiple methods for working easily with dates. This will be specially important in this series since we'll deal with a lot of time series analyses.
	- [`purr`](https://www.rdocumentation.org/packages/purrr/versions/0.2.4): Provides various enhancements to R's functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors.
- Statistical Analysis
	- [`car`](https://cran.r-project.org/web/packages/car/index.html): Provides a set of common & advanced statistical functions. We'll use this when we get to the statistical analysis part of this segment. 
	- [`broom`](https://cran.r-project.org/web/packages/broom/index.html): Summarizes key information about statistical objects in tidy [tibbles](https://tibble.tidyverse.org/). This makes it easy to report results, create plots and consistently work with large numbers of models at once.
- Data visualization
	- [`ggplot2`](https://ggplot2.tidyverse.org/): `ggplot2` is a system for declaratively creating graphics, based on [The Grammar of Graphics](https://www.amazon.com/Grammar-Graphics-Statistics-Computing/dp/0387245448/ref=as_li_ss_tl). This will be our core data visualization package.
	- [`ggalt`](https://cran.r-project.org/web/packages/ggalt/index.html): A compendium of new geometries, coordinate systems, statistical transformations, scales and fonts for `ggplot2`. We'll use this to complement our visualizations.
	- [`RColorBrewer`](https://cran.r-project.org/web/packages/RColorBrewer/index.html): Provides color schemes for maps (and other graphics) designed by Cynthia Brewer as described at http://colorbrewer2.org. We'll use this to create beautiful & consistent color maps.
	- [`viridis`](https://cran.r-project.org/web/packages/viridis/index.html): Provides color maps designed to improve graph readability for readers with common forms of color blindness and/or color vision deficiency. We'll use this to complement the color maps provided by `RColorBrewer`.
	- [`extrafont`](https://cran.r-project.org/web/packages/extrafont/index.html): Provides tools to using fonts other than the standard PostScript fonts. We'll use this package for customizing our visualizations.

Note: We'll install the [`tidyverse`](https://www.tidyverse.org/) package which already includes:
- `ggplot2`
- `dplyr`
- `tidyr`
- `readr`
- `tibble`
- `stringr`
- `purr`
- `forcats`

We'll create our first R file inside our Project Root folder. In this file, we'll declare the packages we'll use. We'll install them & import them. The script will be called `dependencies.R` and will contain the following:

```R
# Define packages to install
required_packages <- c("tidyverse",
                       "readxl",
                       "arrow",
                       "writexl",
                       "openxlsx",
                       "data.table",
                       "car",
                       "broom",
                       "ggalt",
                       "RColorBrewer",
                       "extrafont",
                       "viridis")

for (package in required_packages) {
  # We will check if the package is loaded (hence installed)
  if (!require(package, character.only = TRUE)) {
    install.packages(package)
  }
}

# Snapshot the environment with renv
renv::snapshot()
```

This file will behave as a dependency control file, where each time we need to make a change to a specific package, we'll be able to do so from here, without having to manage dependencies from the analysis scripts we'll be creating. For now, let us simply execute this script; all the packages should be installed in our environment, and the environment snapshot should be generated. This ensures that we freeze the package versions we will install now.

Now that R is set up, we're ready to continue with Python.

### 2.2 Python (PENDING)

Install `virtualenv` package:
```PowerShell
pip install virtualenv
```

Create virtual env:
```PowerShell
cd the-state-of-our-world-in-2023\src\Python; virtualenv venv --python='C:\Users\Pablo\AppData\Local\Programs\Python\Python310\python.exe'
```

Create `requirements.txt`:

```
pandas
numpy
xlsxwriter
scipy
matplotlib
seaborn
scikit-learn
statsmodels
tensorflow
ipykernel
```

Install requirements
```PowerShell
 .\venv\Scripts\Activate.ps1; pip install -r requirements.txt; deactivate
```

---

# A brief introduction to R for Data Science
As we mentioned, this series will be heavily focused on reading & transforming data, producing descriptive statistics, designing statistical experiments, and creating visualizations using R. Because of this, we'll quickly go over the basics under the context of Data Science. We will not go over the absolute basics of R, since that is out of the scope of this series, however a comprehensive beginner's tutorial can be found [here](https://www.statmethods.net/r-tutorial/index.html). You can skip to the **Building the country hierarchy** section if you're already familiar with R for Data Science.

## What is R?
R is a dynamically-typed functional programming language designed by statisticians for statisticians. Unlike languages such as Python, Java or C++, R is not a general purpose language. Instead, it's a domain-specific language (DSL), meaning its functions and use are designed for a specific area of use or domain. In R's case, that's statistical computing and analysis.

So why R you might ask? Why not stick with Python if R is so specific?

Well, Python was not designed to be a statistical programming language; a good deal of the more advanced statistical methods are not available in the base packages or could even not be available at all. Even simpler, data visualization, a crucial component in scientific computing, is by far stronger in R with the implementation of `ggplot`. And some might even argue that data transformation operations are easier & more intuitive in R, but that discussion is so tainted already that I'll leave it for you to decide on that.

All things said, one thing is clear under our specific context: R excels at statistical computing & visualization, and Python excels at advanced Machine Learning algorithms. And if we combine both, we have practically everything we need in order to:
- Import data.
- Validate data.
- Transform & reshape data.
- Perform descriptive statistics.
- Implement advanced classical statistical methods.
- Implement advanced Machine Learning algorithms.
- Visualize our data.

## Importing modules
Once we have a library installed, importing it in R is straightforward. We can use the `library()` function in order to import the library to our current project:

##### **Code**
```R
library(tidyverse)
```

This will make all `tidyverse` methods available to our session.

Let us load the following libraries we'll be using for this introduction to R:

##### **Code**
```R
# Load libraries
library(tidyverse)
library(readxl)
library(data.table)
library(ggalt)
library(RColorBrewer)
library(viridis)
```

## An introduction to functions
Functions are key in maintaining our code modular, and saving us a lot of time when preprocessing & transforming similar datasets. A function in R works just similar to a function in Python or in Scala.

A function will:
- **Accept zero or more inputs**:
	- Functions in R can be designed to take any number of inputs, known as arguments or parameters. These inputs can be mandatory or optional. Functions without any inputs are also valid and are often used for tasks that don't require external data, like generating a specific sequence or value.
	- Functions in R can also accept other functions as inputs, as we'll see in a moment.
- **Perform a given set of instructions**:
	- The body of the function contains a series of statements in R that perform the desired operations. This can include data manipulation, calculations, plotting, other function calls, etc. The complexity of these instructions can range from simple one-liners to extensive algorithms.
- **Return zero or more outputs**:
	- An R function can return multiple values in various forms, such as a single value, a vector, a list, a data frame, or even another function. If a function doesn't explicitly return a value using the `return()` function, it will implicitly return the result of the last expression evaluated in its body. Functions that are intended for side effects, like plotting or printing to the console, might not return any meaningful value (implicitly returning `NULL`).

Additionally, R functions have features like:
- **Scope**: Variables created inside a function are local to that function unless explicitly defined as global.
- **Default Arguments**: Functions can have default values for some or all their arguments, making them optional.
- **Lazy Evaluation**: Arguments in R are lazily evaluated, meaning they are not computed until they are actually used inside the function.
- **First-class Objects**: Functions in R are first-class objects, meaning they can be treated like any other R object. They can be assigned to variables, passed as arguments to other functions, and even returned from other functions.

## Declaring custom functions
We can declare custom functions in a similar fashion from what we would do in Python:

##### **Code**
```R
# Declare a simple function with no arguments and a return statement
myfun_1 <- function() {
  myvar_1 <- 1
  myvar_2 <- 2
  return(myvar_1 + myvar_2)
}

# Call function
myfun_1()

# Declare a simple function with arguments and a return statement
myfun_2 <- function(x, y, z) {
  return(x * y * z)
}

# Call function
myfun_2(2, 3, 4)

# Declare a function accepting another function as argument, and a return statement
inside_fun <- function(x, y) {
  return(x * y)
}

outside_fun <- function(myfun, x, y) {
  # Use inside fun
  return(myfun(x, y))
}

outside_fun(inside_fun, 2, 4)
```

##### **Output**
```
[1] 3
[1] 24
[1] 8
```

## Reading data
In R we can import data to our workspace by reading it from a physical file, by using an API, or by using one of the included R datasets. In our case we'll focus on reading data from existing files, specifically `csv`, `xlsx` & `parquet` files (*the latter we'll leave for later, since we'll not be using it for the initial steps*). Once we read a file in R, we can save our data in different data objects, depending on our requirements. There are three main data objects we'll use in this series:
- `data.frame`: They are tightly coupled collections of variables which share many of the properties of matrices and of lists, used as the fundamental data structure by most of R's modeling software.
- `data.table`: They are an enhanced version of a `data.frame` object, which is the standard data structure for storing data in `base` R.
- `tibble`: A `tibble` or `tbl_df` is an object based on the `data.frame` object, but with some improvements:
	- They don’t change variable names or types.
	- They don’t do partial matching.
	- Are stricter in terms of non-existing variables.
	- Have an enhanced `print()` method.

Some notes:
- A `data.table` object is an extension of a `data.frame` object. Hence, any `data.table` object is also a `data.frame` object.
- This is relevant since, as we'll see, many of the methods applying for `data.frame` objects are similar and sometimes equal to the ones to its enhanced version.

There are four main methods we'll be using to read from the defined file types to the defined objects:
- Reading `csv` files into `data.frame` objects.
- Reading `csv` files into `data.table` objects.
- Reading `xlsx` files into `data.frame` objects.
- Reading `xlsx` files into `data.frame` objects and transforming to `data.table` objects.

### Reading csv files into data.frame objects
We can read `csv` files into `data.frame` objects using the following syntax:

##### **Code**
```R
# Define directories
rDir <- "data/raw/"
wDir <- "outputs"
```

##### **Code**
```R
# Load data using read.csv (Dataframe)
df_csv_dataframe <- read.csv(file.path(rDir,
                                       "GDP_Per_Capita",
                                       "API_NY.GDP.PCAP.PP.CD_DS2_en_csv_v2_6011310.csv"))

# Check object type
class(df_csv_dataframe)
```

##### **Output**
```
[1] "data.frame"
```

### Reading csv files into data.table objects
We can also read `csv` files into `data.table` objects using the following syntax:

##### **Code**
```R
# Load data using fread (data.table)
df_csv_datatable <- fread(file.path(rDir,
                                    "GDP_Per_Capita",
                                    "API_NY.GDP.PCAP.PP.CD_DS2_en_csv_v2_6011310.csv"),
                          header=TRUE)

# Check object type
class(df_csv_datatable)
```

##### **Output**
```
[1] "data.table" "data.frame"
```

### Reading xlsx files
We can read `xlsx` files into `data.frame` objects using the following syntax:

##### **Code**
```R
# Load data using read_excel (Dataframe)
df_xlsx_dataframe <- read_excel(file.path(rDir,
                                          "Drug-Related_Crimes",
                                          "10.1._Drug_related_crimes.xlsx"),
                                sheet="Formal contact")

# Check object type
class(df_xlsx_dataframe)
```

##### **Output**
```
[1] "tbl_df"     "tbl"        "data.frame"
```

We can also transform our `data.frame` object to a `data.table` object using the following syntax:

##### **Code**
```R
# Transform data.frame to data.table from excel-read file
df_xlsx_datatable <- as.data.table(df_xlsx_dataframe)

# Check object type
class(df_xlsx_datatable)
```

##### **Output**
```
[1] "data.table" "data.frame"
```

Once we have our data objects available, we can get information about our data and transform our objects using different operations.

## An introduction to dplyr
[`dplyr`](https://dplyr.tidyverse.org/) is a package included in [`tidyverse`](https://www.tidyverse.org/), a collection of R packages specifically designed for data science-related tasks. In the words of the maintainers, `dplyr` is a grammar of data manipulation, providing a consistent set of verbs that help solve the most common data manipulation challenges (*it's important to mention that, while `dplyr` provides alternative methods to several base R implementations, it does not completely substitute it in most cases, so a hybrid approach is sometimes recommended*):
- `mutate()`: Adds new variables that are functions of existing variables
- `select()`: Picks variables based on their names.
- `filter()`: Picks cases based on their values.
- `summarize()`: Reduces multiple values down to a single summary.
- `arrange()`: Changes the ordering of the rows.

All of the operations above can be used with the `group_by()` method, which allows us to perform any operation “by group”.

Additionally, we also have the [mutation join operations](https://dplyr.tidyverse.org/reference/mutate-joins.html):
- `inner_join()`: Returns matched `x` rows.
- `left_join()`: Returns all `x` rows.
- `right_join()`: Returns matched of `x` rows, followed by unmatched `y` rows.
- `full_join()`: Returns all `x` rows, followed by unmatched `y` rows.

Below are some key points regarding `dplyr`:
- It natively works with `data.frame` objects.
- When possible, the data object will always be the first argument in any method.
- When possible, the output of a method will be a `data.frame`. This allows us to pipe methods in a sequential manner, as we'll see in a minute.

### The dplyr verbs
`dplyr` at its core supports 5 verbs or transformations. We already mentioned them in the previous section, but here we'll take a more detailed look at how to use them.

#### select()
[`select()`](https://dplyr.tidyverse.org/reference/select.html) will be responsible for transforming columns (*variables*) based on their names. With `select()` we can:
- Extract a column.
- Reorder columns.
- Rename columns.
- Remove columns.

#### filter()
[`filter()`](https://dplyr.tidyverse.org/reference/filter.html) is used on a per-row basis to subset a data frame. With `filter()` we can:
- Remove `NaN` values.
- Filter by entry name on a given column.
- Filter by using comparison & logical operators.
- Filter by using other methods such as:
	- `is.na()`
	- `between()`
	- `near()`
- Filter by using our own functions for evaluation.
- And much more.

#### mutate()
[`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html) is used to apply transformations to existing columns in our data frame. The result can then saved to the original column or added as a new column to our data frame. With `mutate()` we can:
- Apply a mathematical function to an existing column.
- Assign a rank based on a given column's values.
- Apply transformations using conditional constructs.
- And much more.

#### arrange()
[`arrange()`](https://dplyr.tidyverse.org/reference/arrange.html) is used to order the rows of a data frame by the values of selected columns.

#### summarize()
[`summarize()`](https://dplyr.tidyverse.org/reference/summarise.html) is used to perform summary statistics on a column basis. With `summarize()` we can:
- Get the mean of the values on a given column.
- 

## Data transformations & dplyr
`dplyr` is strongest in the data transformation area. We can combine verbs & methods in order to do practically most of the transformation & preprocessing work we need for this series. Let us now illustrate `dplyr`'s capabilities with some hands-on examples.

### Transformations using a single verb
We can start by illustrating how a transformation with a single verb can be defined:

##### **Code**
```R
# Get a single column
metrics_2005 <- df_csv_dataframe %>%
  select('X2005')
  
# Check head
head(metrics_2005)
```

As we can see:
- We start by declaring a new variable `metrics_2005`, which will accept the results of the computation.
- We then refer to the data frame object `df_csv_dataframe`. This will be the starting point of our transformation.
- We then use the `select()` verb in order to extract a specific column.
- Hence, the output will be of the same type as the input, with only one column (`x2005`) as the selected column.
- Finally, we print the head of the object.

##### **Output**
```
      X2005
1 36264.435
2  2635.508
3  1075.671
4  2780.209
5  4835.670
6  5865.291
```

We can also remove `NaN` entries and add a new column that adds up all values on our single-column dataframe:

##### **Code**
```R
# Filter NA
metrics_2005 <- metrics_2005 %>%
  filter(!is.na(X2005))

# Mutate using arithmetic operations
metrics_2005 <- metrics_2005 %>%
  mutate(sum_2005 = sum(X2005))

# Check head
head(metrics_2005)
```

##### **Output**
```
      X2005 sum_2005
1 36264.435  3341552
2  2635.508  3341552
3  1075.671  3341552
4  2780.209  3341552
5  4835.670  3341552
6  5865.291  3341552
```

But wouldn't it be nice if we could do this on a single expression? I mean, assigning & reassigning variables is tedious. Well, turns out `dplyr` has an implementation that solves exactly this problem.

### Transforming using multiple verbs (*the pipe operator*)
Complex data transformations using single expressions with `dplyr` can be achieved by using the pipe operator `%>%`. People with functional programming backgrounds and SQL users will find this approach very familiar; at its core, the pipe operator serves as an interface between one input and one output. It accepts one input, and evaluates the next function with it:

```
... output input -> %>% -> output input -> %>% -> output ...
```

So in this way, the output of a function automatically becomes the input of another function without having to assign any intermediate variables.

Let us start by doing a three-step transformation using the pipe operator. We want to use the `df_csv_dataframe` in order to:
- Only select the base columns & latest year
- Remove `NaN` entries for the latest year
- Create an additional column that calculates the mean of the `X2022` column.

##### **Code**
```R
# Perform transfromation
df_2022 <- df_csv_dataframe %>%
  select(c(Country.Name,
           Country.Code,
           Indicator.Name,
           Indicator.Code,
           X2022)) %>%
  filter(!is.na(X2022)) %>%
  mutate(mean_2022 = mean(X2022))
```

##### **Output**
```
                 Country.Name Country.Code                                Indicator.Name    Indicator.Code     X2022 mean_2022
1 Africa Eastern and Southern          AFE GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD  4169.020  25187.34
2  Africa Western and Central          AFW GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD  4798.435  25187.34
3                      Angola          AGO GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD  6973.696  25187.34
4                     Albania          ALB GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 18551.716  25187.34
5                  Arab World          ARB GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 16913.653  25187.34
6        United Arab Emirates          ARE GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 87729.191  25187.34
```

Note that:
- We can break lines at the end of each pipe operator, making the expression highly readable.
- We can use vectors of column names inside `select()`.

### Calculating aggregations
https://medium.com/@serwaabaah/easily-aggregate-data-using-the-dplyr-function-in-r-ed694d064a94

### Transforming using other functions
We can also pipe our data to other functions such as:
- [`unique()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/unique): Returns a vector, data frame or array like entry object but with duplicate elements/rows removed.
- [`count()`](https://www.rdocumentation.org/packages/dplyr/versions/1.0.10/topics/count): Returns an object of the same type as the entry object with the count of unique values of one or more variables.
- [Custom functions](https://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/functions.pdf): We can declare a custom function and pass it as argument in our chain of transformations.
- And much more

We can, for example, return the unique number of items from a specific column in our dataframe:

##### **Code**
```R
# Perform additional operations
unique_codes <- df_csv_dataframe %>%
  select(Country.Code) %>%
  unique() %>%
  arrange(desc(.))

# Check head
head(unique_codes)
```

Note:
- We call `desc()` with a dot `.` as argument. The use of the dot `.` is a feature of the `dplyr` package which acts as a placeholder for the data being passed through the pipeline. In short, we're telling R "*arrange the data in descending order based on the data itself*". Since the data at this point is just a single column, it effectively means "*arrange the unique country codes in descending order*". This is extremely useful when we don't want to rewrite the entire column name again.

##### **Output**
```
  Country.Code
1          ZWE
2          ZMB
3          ZAF
4          YEM
5          XKX
6          WSM
```

We can even include the `head()` statement as part of the pipe sequence. This way we can get the top *n* items ordered alphabetically on descending order:

##### **Code**
```R
# Get the top 10 items ordered alphabetically on descending order
unique_codes_head <- df_csv_dataframe %>%
  select(Country.Code) %>%
  unique() %>%
  arrange(desc(.)) %>%
  head(., 10)

# Check object
unique_codes_head
```

##### **Output**
```
   Country.Code
1           ZWE
2           ZMB
3           ZAF
4           YEM
5           XKX
6           WSM
7           WLD
8           VUT
9           VNM
10          VIR
```

## Data tidying & tidyr
As mentioned previously, `tidyr` is also included in the `tidyverse` package. It's main use case is to create tidy data. [According to the official documentation](https://tidyr.tidyverse.org/), tidy data is data where:
- Every column is a variable.
- Every row is an observation.
- Every cell is a single value.

The importance of using tidy data is that most of the times when using some of the `tidyverse` packages, functions will work with this format by default, and will require additional maneuvers when we present other forms. For example, let us come back to the following dataset:

 ##### **Code**
 ```R
 colnames(df_csv_dataframe)
```

##### **Output**
```
"Country.Name",
"Country.Code",
"Indicator.Name",
"Indicator.Code",
"X1960",
"X1961",
"X1962",
...
```

As we can see, we have the following columns:
- `Country.Name`: Identifies the observational unit (*country*), which is appropriate.
- `Country.Code`: Identifies the observational unit (*country*), which is appropriate.
- `Indicator.Name`: Identifies another aspect of the observation, which is also appropriate.
- `Indicator.Code`: Identifies another aspect of the observation, which is also appropriate.
- `X1960`, `X1961`, `X1962`, ..., `X196n`: Represent values of a variable over time (*years*), which is not appropriate.

The problem is that "*year*" is supposed to be a variable; instead, we're saying that each year is a variable in and of itself. In its current form, our dataframe does not adhere to the principles of a "*tidy*" dataframe as defined by the `tidyverse` philosophy.

This dataset format is fine when we're performing simple calculations using `dplyr` or even Base R; however, the problem is more evident when visualizing our data:
- `ggplot2` is designed with the assumption that data is tidy. This design makes it straightforward to map variables to aesthetic attributes like x and y axes, color, size, etc.
- Tidy data allows for more flexible and complex visualizations. We can easily switch between different types of plots or layer multiple types of visualizations together without significant restructuring of your data.
- Tidy data simplifies the process of creating facets (*subplots based on data subsets*) or grouping data.

If our data is not tidy:
- We'll generally still be able to plot, but there might be limited features, specially for more advanced plotting.
- We'll most probably be doing extensive data transformations inside the plotting code, which is far from ideal.

So in short, using tidy data in `tidyverse` is always recommended; we'll always use tidy data in this series.

### Generating tidy data
`tidyr` has a ton of functions; we'll not be using all of them of course. In this series, we'll mostly be using the following:
- `pivot_longer`: Transform from wide to long.
- `pivot_wider`: Transform from long to wide.

Let us start with a simple example, where we would like to tidy our `df_csv_dataframe`. In short, we need to:
- Take all columns containing yearly entries.
- Convert them to a single column called `year`.
- For each observation, expand vertically in order to accommodate all years in a single column.

And the best of all is that we can use `tidyr` functions as `dplyr` transformation steps, making this a breeze to work with:

##### **Code**
```R
# Tidy data (use year cols)
df_csv_dataframe_longer <- df_csv_dataframe %>%
  pivot_longer(cols = starts_with('X'),
               names_to = "year")

# Check head
head(df_csv_dataframe_longer)
```

Let us break this down in more detail:
- The first argument for `pivot_longer` will always be the data structure. However, we're piping using `dplyr` syntax so this is not required.
- The second argument is the column set we would like to transform to a single variable, in this case all columns starting with the character 'X'.
- The third argument is the name of the column that will store our converted observations (*which were previously single variables*). By default, R will set this as `name`.
- The fourth argument indicates the name of the column that will contain the values. By default, R will set this as `value`.

##### **Output**
```
  Country.Name Country.Code Indicator.Name                                Indicator.Code    Year  Value
  <chr>        <chr>        <chr>                                         <chr>             <chr> <dbl>
1 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD X1960    NA
2 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD X1961    NA
3 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD X1962    NA
4 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD X1963    NA
5 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD X1964    NA
6 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD X1965    NA
```

Well this is nice, but how do we get rid of the X prefix? We don't want this in our dataset, right? Turns out `tidyr` devs thought of this already, and implemented a nice `names_prefix` argument we can use to get rig of any prefix in our names column:

##### **Code**
```
# Tidy data (use year cols)
df_csv_dataframe_longer <- df_csv_dataframe %>%
  pivot_longer(cols = starts_with('X'),
               names_to = "Year",
               values_to = "Value",
               names_prefix = 'X')


# Check head
head(df_csv_dataframe_longer)
```

##### **Output**
```
  Country.Name Country.Code Indicator.Name                                Indicator.Code    Year  Value
  <chr>        <chr>        <chr>                                         <chr>             <chr> <dbl>
1 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1960     NA
2 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1961     NA
3 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1962     NA
4 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1963     NA
5 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1964     NA
6 Aruba        ABW          GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1965     NA
```

Fair, but what if we don't have the 'X' prefix to work with in the first place? Since we're counting on it in order to know which columns to convert to observations, we'll have to perform some previous steps first:

##### **Code**
```R
# Create a new df_csv_dataframe from our data.table object
# data.tables do not automatically prepend numeric col names with X
df_csv_dataframe <- as.data.frame(df_csv_datatable)

# Declare base cols (we'll respect these)
base_cols <- c("Country Name",
               "Country Code",
               "Indicator Name",
               "Indicator Code")

# User pivot_longer without column prefixes
df_csv_dataframe_longer <- df_csv_dataframe %>%
  pivot_longer(cols = !all_of(base_cols),
               names_to = "Year",
               values_to = "Values")

# Check head
head(df_csv_dataframe_longer)
```

##### **Output**
```
  `Country Name` `Country Code` `Indicator Name`                              `Indicator Code`  Year  Values
  <chr>          <chr>          <chr>                                         <chr>             <chr>  <dbl>
1 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1960      NA
2 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1961      NA
3 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1962      NA
4 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1963      NA
5 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1964      NA
6 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1965      NA
```

Let us break this down in more detail:
- We first make a copy of our `data.table` object so that we get the year names without the prepended 'X' character.
- We then define a new vector `base_cols`. This object will contain the columns that we want to keep intact when the transformation is executed.
- Lastly, we call `pivot_longer`, but instead of indicating a set of columns to pivot, we declare all columns except the base columns.
- The rest stays exactly the same.

### Generating untidy data
Of course, we can do the inverse operation as we did in the last example, and make our data object wider by pivoting column values as new variables. This comes in handy when we get a dataset that has the following columns:
- Variable: A set of variables.
- Value: A set of values corresponding to each of the variables.

When in reality we should have:
- One variable per column.
- One observation per row.

Let us try to do an example:

##### **Code**
```R
# Declare Dataframe
# Declare variables column 
df_variables <- c("Height",
                  "Width",
                  "X Coordinate",
                  "Y Coordinate")

# Declare values column
df_values <- c(10,
               20,
               102.3,
               102.4)

# Declare frame using vectors as columns
df_long <- data.frame(variables = df_variables,
                      values = df_values)

# Check head
head(df_long)
```

##### **Output**
```
     variables values
1       Height   10.0
2        Width   20.0
3 X Coordinate  102.3
4 y Coordinate  102.4
```

So what we need to do is convert every variable to a new column, and set each value in a row. Each row would be a new observation, but this dataframe only has one observation (*let us call it object A*). Hence, our object will only contain 1 row:

##### **Code**
```R
# Pivot wider in order to get tidy object
df_wider <- df_long %>%
  pivot_wider(names_from = variables,
              values_from = values)

# Check head
head(df_wider)
```

##### **Output**
```
  Height Width `X Coordinate` `y Coordinate`
   <dbl> <dbl>          <dbl>          <dbl>
1     10    20           102.           102.
```

And that's it, we've transformed our original dataset into a tidy set by using `tidyr` & `dplyr`.

Now that we have tidy data, we can start doing some statistical calculations.

## Merging objects
Merging is a crucial part of data manipulation, and we'll be doing a lot in this series. Merging works very similar to other languages such as Python. There are different types of merge operations, depending on the underlying set theory implementation we'd like to use.

Merging in R can be executed using multiple libraries. However, we'll use three in this series:
- Base R
- `dplyr`
- `tidyverse`

### Merging using Base R
Base R provides us with a `merge()` function. We can use it to perform the following join operations:
- outer
- left
- right
- inner

However, the function is the same, and what changes in this case is the join argument we pass:

##### **Code**
```R
# Using Base R merge()
# Define two frames
df_countries <- data.frame(country_key = c("SEN", "LTU", "ISL", "GBR"),
                           country = c("Senegal", "Lituania", "Iceland", "United Kingdom"))

df_metrics_1 <- data.frame(country_key = c("LTU", "MEX", "GBR", "SEN", "ISL"),
                         metric = c(23421, 234124, 25345, 124390, 34634))

df_metrics_2 <- df_metrics <- data.frame(country_key = c("LTU", "SEN", "CHE", "GBR", "ISL"),
                                         metric = c(37824, 89245, 28975, 49584, 29384))

# Create new frame merging left for df_countries on key
df_left_base <- merge(df_countries, df_metrics_1, by = "country_key", all.x = TRUE)

# Check head
head(df_left_base)

# Create new frame merging right for df_metrics on key
df_right_base <- merge(df_countries, df_metrics_1, by = "country_key", all.y = TRUE)

# Check head
head(df_right_base)

# Create new frame merging inner on key
df_inner_base <- merge(df_countries, df_metrics_1, by = "country_key", all = FALSE)

# Check head
head(df_inner_base)

# Create a new frame by merging df_left_base with df_metrics_2 using left_merge
df_left_base <- merge(df_left_base, df_metrics_2, by = "country_key", all.x = TRUE)

# Check head
head(df_left_base)
```

##### **Output**
```
  country_key        country metric
1         GBR United Kingdom  25345
2         ISL        Iceland  34634
3         LTU       Lituania  23421
4         SEN        Senegal 124390

  country_key        country metric
1         GBR United Kingdom  25345
2         ISL        Iceland  34634
3         LTU       Lituania  23421
4         SEN        Senegal 124390

  country_key        country metric.x metric.y
1         GBR United Kingdom    25345    49584
2         ISL        Iceland    34634    29384
3         LTU       Lituania    23421    37824
4         SEN        Senegal   124390    89245
```

### Merging using dplyr
We already saw that `dplyr` has very nice transformation functions. Additionally, it also provides merge functions. However, these vary with regards to the join operation we're trying to perform:

##### **Code**
```R
# Using dplyr 
df_left_dplyr <- df_countries %>%
  left_join(df_metrics_1, by = "country_key") %>%
  inner_join(df_metrics_2, by = "country_key")

# Check head
head(df_left_dplyr)
```

##### **Output**
```
  country_key        country metric.x metric.y
1         SEN        Senegal   124390    89245
2         LTU       Lituania    23421    37824
3         ISL        Iceland    34634    29384
4         GBR United Kingdom    25345    49584
```

### Merging using purr
At last we have the [`reduce()`](https://www.rdocumentation.org/packages/purrr/versions/0.2.4/topics/reduce) function from the `purr` package. The `reduce()` function is not just a merge function; it's much more than that. The `reduce()` function is a commonly-used method in functional programming. In fact, it's one of the key functions of any FP language along with `map()`, and `mapreduce()`.

The `reduce()` function will reduce a list of objects to a single object by iteratively applying a function. We have two flavors:
- `reduce()`: Will reduce from left to right (*we'll be mostly using this one*)
- `reduce_right()`: Will reduce from right to left.

Let us put `reduce()` in practice by using our previous examples:

##### **Code**
```R
# Using purr's reduce() function
df_list <- list(df_countries,
                df_metrics_1,
                df_metrics_2)

df_left_reduce <- df_list %>%
  reduce(left_join, by = "country_key")

# Check head
head(df_left_reduce)
```

##### **Output**
```
  country_key        country metric.x metric.y
1         SEN        Senegal   124390    89245
2         LTU       Lituania    23421    37824
3         ISL        Iceland    34634    29384
4         GBR United Kingdom    25345    49584
```

This approach has three main advantages that we'll greatly leverage throughout this series:
- We can merge n number of objects using one single expression.
- It fits seamlessly with `dplyr`.
- We can use any function, including custom functions (*of course, not just necessarily for merging*).

## Classical statistics
This mini tutorial would not be complete without going over some of the very basic classical statistics concepts in R, given that R is a statistics-oriented programming language and we'll be making extensive use of a lot of the methods included. Please keep in mind that this is in no way a comprehensive review of classical statistics. Instead, it's a very quick mention on the most relevant methods in R, and how we'll implement them in our analyses further on.

### Descriptive Statistics
Descriptive statistics are a key aspect in Data Science & Data Analysis. They provide ways of summarizing & describing data, and are used extensively in Exploratory Data Analysis (*EDA*) since many times it provides a friendly & comprehensive entry point where our data is extensive. They can also be used:
- Aa
- Aa
- To monitor processes when a lot of data is being gathered in real time.

In the words of [Jennifer L. Green et al.](https://www.sciencedirect.com/topics/social-sciences/descriptive-statistics) descriptive statistics provide summarizing information of the characteristics and distribution of values in one or more datasets. The classical descriptive statistics allow analysts to have a quick glance of the central tendency and the degree of dispersion of values in datasets. They are useful in understanding a data distribution and in comparing data distributions.

Descriptive statistics can be categorized based on what they measure:

- Measures of Frequency
	- **Examples**: Count, Percent, Frequency
	- **Objective**: Shows **how often** something occurs
- Measures of Central Tendency
	- **Examples**: Mean, Median, and Mode
	- **Objective**: Locates the **distribution** by various points
- Measures of Dispersion or Variation
	- **Examples**: Range, Variance, Standard Deviation
	- **Objective**: Identifies the **spread** of scores by stating intervals
- Measures of Position
	- **Examples**: Percentile Ranks, Quartile Ranks
	- **Objective**: Describes how scores fall in relation to one another. Relies on standardized scores.

R has implementations for all of the statistics above. They're simple to use and can also be obtained using summarization functions:

##### **Code**
```R
# Calculate summary stats for specific columns
summary(df_csv_dataframe$`2022`)

# Calculate single statistics for a given column
mean(df_csv_dataframe$`2022`, na.rm = TRUE)
median(df_csv_dataframe$`2022`, na.rm = TRUE)
sqrt(var(df_csv_dataframe$`2022`, na.rm = TRUE))
sd(df_csv_dataframe$`2022`, na.rm = TRUE)
quantile(df_csv_dataframe$`2022`, na.rm = TRUE)
sum(is.na(df_csv_dataframe$`2022`))
```

##### **Output**
```
Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's 
836.2   6004.2  17007.3  25187.3  39448.8 142213.9       34

[1] 25187.34

[1] 17007.34

[1] 25677.39

[1] 25677.39

0%         25%         50%         75%        100% 
836.1876   6004.1637  17007.3438  39448.8042 142213.8517

[1] 34
```

As we can see here, `summary()` will give us the most relevant summary statistics, which we can also calculate individually.

We can also use visualizations such as boxplots & histograms in order to get more than one summary statistic visually. We'll see this in more detail when we get to Plotting.

### Probability Distributions
In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. The most common distributions are:
- Normal (*Gaussian*)
- Binomial
- Uniform
- Poisson
- Exponential

Visual representations of the PDFs (*Probability Density Functions*) for the most relevant distributions can be found [here](https://www.fil.ion.ucl.ac.uk/~wpenny/course/appendixDE.pdf)

Although we'll not be using distributions in this series as strongly as descriptive statistics, we'll still employ them when we start making more advanced analyses. Because of this, it's relevant to see how we can use them in R.

We can create a set of random numbers drawn from any given probabilistic distribution:

##### **Code**
```R
# Create sets of random numbers drawn from distributions
# Declare target number of observations
n <- 10

# Normal
rnorm(n = n, mean = 0, sd = 1)

# Binomial
rbinom(n = n, size = 50, prob = 0.5)

# Uniform
runif(n = n, min = 0, max = 1)

# Poisson
rpois(n = n, lambda = 5)

# Exponential
rexp(n = n, rate = 1)
```

##### **Output**
```
[1]  1.3182636  1.0737835 -0.2168472 -0.4819734  0.7021963 -0.9281154  1.8605865  0.8082168 -0.5036910 -1.3031976

[1] 26 21 27 34 21 22 28 26 28 25

[1] 0.50290228 0.34901311 0.67150315 0.62351605 0.70979582 0.06286753 0.11379842 0.38474496 0.53878275 0.79491283

[1]  5  5  3  4  5  7  8 10  7  5

[1] 0.0581296 0.7856057 2.5060449 2.0419846 0.5936905 0.1878208 2.4909209 1.4005595 3.4965515 0.4754702
```

### Correlation and Regression Analysis
Lastly, we discuss correlation & regression analysis. In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. Correlations are extremely useful when we're trying to find relationships between one variable and another in order to:
- Understand phenomena
- Make predictions

Two or more variables can have some type or no relation between each other:
- Positive: Relationship goes in the same direction.
- Negative: Relationship goes in opposite directions.
- No Correlation: There is no correlation between variables.

We measure the degree of relationship by calculating correlation coefficients. These vary depending on the variables we're studying. Below are some examples:
- **Pearson Product-Moment Correlation Coefficient**:
	- Parametric.
	- It measures the strength and direction of the linear relationship between two variables, and is defined as the covariance of the variables divided by the product of their standard deviations.
- **Spearman’s Rank Correlation Coefficient (_Kendall’s tau-b_)**:
	- Non-parametric.
	- It measures how well the relationship between two variables can be described by a [monotonic function](https://mathworld.wolfram.com/MonotonicFunction.html).
- **Kendall Rank Correlation Coefficient**:
	- Non-parametric.
	- It measures the ordinal association (*rank correlation*) between two measured quantities.

Each coefficient has assumptions that in theory need to be met in order for us to be able to use it under that specific scenario. If the conditions are not met, we can probably use an alternative coefficient.

We'll use correlation studies throughout this series in order to:
- Understand the relationship between metrics.
- Make predictions based on potential relationships. 

We'll briefly take a loot at the basics of correlational analysis using the Swiss Dataset.

First, we load the required dataset and check properties:

##### **Code**
```R

```

##### **Output**
```

```

## Plotting
Generating visuals in R is slightly different than doing the same in Python. R has some core components we need to familiarize ourselves with:
- **Layer**: A layer combines data, aesthetic mapping, a geom (*geometric object*), a stat (*statistical transformation*), and a position adjustment.
	- **Data**: Dataset that we wish to use for our visualization.
	- **Mapping**: Aesthetic mappings describe how variables in the data are mapped to visual properties (*aesthetics*) of geoms.
	- **Geoms**: Geometric objects that will be contained in our visual.
	- **Stats**: Stats define statistical transformations that can happen within our visual.
	- **Position**: All layers have a position adjustment that resolves overlapping geoms.
	- **Scales**: Scales control the details of how data values are translated to visual properties. These can include plot labels, axis legends, data labels, and axis scaling.
	- **Faceting**: Faceting generates small multiples, each displaying a different subset of the data. Facets are an alternative to aesthetics for displaying additional discrete variables.
	- **Coordinate Space**: The coordinate system determines how the x and y aesthetics combine to position elements in the plot.
	- **Themes**: Themes control the display of all non-data elements of the plot.

### Plotting a simple line chart
We'll use the `ggplot` library in order to plot our first line chart. We'll use the tidy dataframe we already have in our hands:

##### **Code**
```R
head(df_csv_dataframe_longer)
```

##### **Output**
```
  `Country Name` `Country Code` `Indicator Name`                              `Indicator Code`  Year  Values
  <chr>          <chr>          <chr>                                         <chr>             <chr>  <dbl>
1 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1960      NA
2 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1961      NA
3 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1962      NA
4 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1963      NA
5 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1964      NA
6 Aruba          ABW            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 1965      NA
```

Let us reduce the set a bit so we can work more comfortably:

##### **Code**
```R
# Reduce set by selecting a subset of countries & years
country_scope <- c("BRA",
                   "CHN",
                   "FRA")

year_scope <- as.character(
  seq(2010, 2020)
  )

# Filter dataframe
df_csv_dataframe_longer <- df_csv_dataframe_longer %>%
  filter((Year %in% year_scope) & (`Country Code` %in% country_scope))

# Check head
head(df_csv_dataframe_longer)

# Check shape
count(df_csv_dataframe_longer)
```

##### **Output**
```
  `Country Name` `Country Code` `Indicator Name`                              `Indicator Code`  Year  Values
  <chr>          <chr>          <chr>                                         <chr>             <chr>  <dbl>
1 Brazil         BRA            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 2010  14255.
2 Brazil         BRA            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 2011  14989.
3 Brazil         BRA            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 2012  14994.
4 Brazil         BRA            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 2013  15536.
5 Brazil         BRA            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 2014  15665.
6 Brazil         BRA            GDP per capita, PPP (current international $) NY.GDP.PCAP.PP.CD 2015  14693.

  <int>
1    33
```

Now, we can create a simple time series plot for the 3 countries we previously selected:

##### **Code**
```R
# Check data types first
str(df_csv_dataframe_longer)

# We must first convert Year to numeric
# Note: Here we use Base R syntax since it's more direct for this case
df_csv_dataframe_longer$Year <- as.integer(df_csv_dataframe_longer$Year)

# However, we can also use dplyr syntax
df_csv_dataframe_longer <- df_csv_dataframe_longer %>%
  mutate(Year = integer(Year))

# Generate plot
ggplot(data = df_csv_dataframe_longer,
       mapping = aes(x = Year, y = Values, color = `Country Name`)) +
  geom_line() + 
  theme_gray() + 
  labs(title = "GDP per capita, PPP (current international $)",
       x = "Year",
       y = "Metric",
       color = "Country") + 
  scale_x_continuous(breaks = pretty(df_csv_dataframe_longer$Year, n = 5))
```

Let us explain this in more detail:
- The first argument is always the dataframe, in our case, `df_csv_dataframe_longer`.
- The second argument is `mapping`. As we already mentioned, `mapping` is responsible for assigning variables to components in a `ggplot2` figure. It does this by the help of `aes` (*aesthetics*).
- The next components are added by using a plus sign `+`, and if we pay close attention, are declared after we close the `ggplot` expression. Any additional component will follow this rule.
	- We include a geom, specifically a line, by using `geom_line()`. A complete list of available themes can be found [here](https://ggplot2.tidyverse.org/reference/#geoms).
	- We also include a theme, which in our case is `theme_gray()`. A complete list of available themes can be found [here](https://ggplot2.tidyverse.org/reference/ggtheme.html).
	- Next, we include labels using `labs`. A complete list of possible parameters can be found [here](https://ggplot2.tidyverse.org/reference/labs.html).
	- Lastly, we include a scale for the x axis. This is not required in all cases, however in our case, since we want to have more control over the `Year` variable (*cannot be displayed as decimal*), the `scale_x_continuous` will tell R to use breaks provided by the `pretty(Years, n = 5)` method. In short, it will automatically calculate the years to display based on the required number of breaks (*n = 5*).

##### **Output**
01_time_series.png

### Plotting a boxplot
Boxplots are key when it comes to descriptive statistics. They provide information about the locality, spread and skewness groups of numerical data through their quartiles. We'll be using boxplots & similar visualizations a lot in this series. Thus, we can try to exemplify by using our dataframe:




These are just the very basics for using `ggplot2`, and sometimes this does the trick. Of course, ggplot2 is much more powerful and we'll put it to its limits in this series. Not to worry, we'll discuss more visuals as we navigate through the actual metrics.

Now that we know the very basics of R for Data Science, we can 

---

# Building the country hierarchy
Let us start by building our country hierarchy. We need to define the following:
- Country Names
- Country Codes
- Continent Names
- Region Names
- International Organizations (*if any*)
- Classification by Income

This information can be extracted from [this dataset](https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes/blob/master/all/all.csv) by [lukes](https://github.com/lukes). The `csv` file contains the following columns:
- `name`
- `alpha-2`
- `alpha-3`
- `country-code`
- `iso_3166-2`
- `region`
- `sub-region`
- `intermediate-region`
- `region-code`
- `sub-region-code`
- `intermediate-region-code`

We will use these specific country names throughout the entire series, since some datasets present different names with slight variations, and we want to ensure that the names are consistent throughout our visuals. We will use the [ISO 3166](https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes) country code framework, specifically the Alpha-3 field as key for the merging operations between datasets.

We will also use a mapping of relevant international organizations per each country so that we can later aggregate countries by organizations. This will give us an extra layer for analysis.

Finally, we will also add development classifications to each country. This will establish a baseline in terms of the degree of development, providing yet another extra layer for analysis.

Let us first import the required libraries:

##### **Code**
```R
library(readr)
library(readxl)
library(writexl)
library(openxlsx)
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
```

Next, we'll set some variables: 

##### **Code**
```R
# Directories
rDir <- "data/raw/"
w_FigDir <- "outputs/figures"
utilDir <- file.path(rDir, "Utilities")
```

Now we can read the datasets and perform some transformations so that in the end we have one `data.frame` object with all the relevant fields. We'll start with the country codes:

##### **Code**
```R
# Load country list
df_countries <- read.csv(file.path(utilDir, "Country_Codes.csv"))
colnames(df_countries)

# Change column names
# https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes
df_countries <- df_countries %>%
  rename(
    "country_name" = "name",
    "iso3166_1_alpha_2" = "alpha.2",
    "iso3166_1_alpha_3" = "alpha.3",
    "iso3166_1_numeric_code" = "country.code",
    "iso3166_2_subdiv" = "iso_3166.2",
    "sub_region" = "sub.region",
    "intermediate_region" = "intermediate.region",
    "region_code" = "region.code",
    "sub_region_code" = "sub.region.code",
    "intermediate_region_code" = "intermediate.region.code"
  )

# Create simple country codes df
df_countries <- df_countries %>%
  select(country_name,
         iso3166_1_alpha_3,
         region,
         region_code,
         sub_region,
         sub_region_code)
```

Next, we'll proceed with the international organizations file. The main organizations we'll include are the following:
- UN
- EU
- EEA
- EFTA
- OECD
- IMF
- IBRD
- NATO
- BRICS
- WTO
- Commonwealth
- African Union
- ASEAN
- Arab League
- OPEC
- G7
- G20
- Schengen Area
- Pacific Islands Forum

We'll load our International Organizations Excel file and create new columns in `df_countries` that indicate us weather a given country belongs to the organization under study:

##### **Code**
```R
# Get the name of all Excel sheets
sheet_names <- excel_sheets(file.path(utilDir, "International_Organizations.xlsx"))

# Iterate over each sheet
for(sheet in sheet_names) {
  # Read the sheet
  sheet_data <- read_excel(file.path(utilDir, "International_Organizations.xlsx"), sheet = sheet)
  # Check if countries in df_countries are present in the sheet
  df_countries[[sheet]] <- df_countries$iso3166_1_alpha_3 %in% sheet_data$country_code
}

colnames(df_countries)
```

##### **Output**
```
 [1] "country_name"          "iso3166_1_alpha_2"     "iso3166_1_alpha_3"     "region"                "region_code"          
 [6] "sub_region"            "sub_region_code"       "UN"                    "EU"                    "EEA"                  
[11] "EFTA"                  "OECD"                  "IMF"                   "IBRD"                  "NATO"                 
[16] "BRICS"                 "WTO"                   "commonwealth"          "african_union"         "ASEAN"                
[21] "arab_league"           "OPEC"                  "G7"                    "G20"                   "schengen_area"        
[26] "pacific_islands_forum"
```

Next, we'll load the country classifications file and merge it with our `df_countries` object. We'll only use the classifications for 2022 since most of our analysis will be based on 2022:

##### **Code**
```R
# Load country classification data
df_country_classification <- read_excel(file.path(utilDir, "Country_Development_Classifications.xlsx"))

# Replace inexisting values with NaN
df_country_classification <- df_country_classification %>%
  mutate_all(~na_if(., ".."))

# We'll only use the 2022 classification since most of the metrics will be from 2022
df_country_classification <- df_country_classification[, c("iso3166_1_alpha_3", "2022")] %>%
  rename("country_classification_2022" = "2022")

# Left merge dataframes
df_countries <- merge(df_countries,
                      df_country_classification,
                      by="iso3166_1_alpha_3",
                      all.x = TRUE)
```

Now, we can simply export our `data.frame` object as `csv` file. We'll use it as base for the next section:

##### **Code**
```R
write.csv(df_countries, file.path(utilDir, "Countries_Baseline.csv"), row.names = FALSE)
```

The final dataset can be found [here]().

---

# Macroeconomics
According to Britannica, macroeconomics is the study of the behaviour of a national or regional economy as a whole. It is concerned with understanding economy-wide events such as the total amount of goods and services produced, the level of unemployment, and the general behaviour of prices.

Depending on who we ask, there are around 7 schools of macroeconomic thought. If we base ourselves on Edmund S. Phelps' [Seven Schools of Macroeconomic Thought](https://academic.oup.com/book/2854), we can find the following:
- **Keynesian**: The central tenet of this school of thought is that government intervention can stabilize the economy via monetary policies & other actions.
- **Monetarist**: This school maintains that the money supply (*the total amount of money in an economy, in the form of coin, currency, and bank deposits*) is the chief determinant on the demand side of short-run economic activity.
- **New Classical**: This school takes the view that short-run fluctuations in the aggregate economy—the business cycle—can be understood within an equilibrium framework of rational, forward-looking agents.
- **New Keynesian**: This school is an evolution of the Keynesian movement. It comes with two main assumptions. First, that people and companies behave rationally and with rational expectations. Second, it assumes a variety of market inefficiencies – including sticky wages and imperfect competition.
- **Supply-side**: Focuses on influencing the supply of labour and goods, using tax cuts and benefit cuts as incentives to work and produce goods.
- **Neoclassical & Neo-Classical Real Business Cycle**: This theory suggests that business cycles are a result of technological changes and the availability of resources, both of which influence productivity and cause changes in the long-run aggregate supply.
- **Structuralist**: This school emphasizes the importance of taking into account structural features (*typically*) when undertaking economic analysis.

Macroeconomics is not simply explained by a couple of metrics; we're talking about a complex system, so we need to look at it from different optics. This is where metrics under this context come into place; they help us assess the current state of the macroeconomic system, and evaluate on how to move forward.

Some of the most relevant subjects that can help explain & influence macroeconomic phenomena are:
- Fluctuation in Interest Rates
- Productive Output (GPD)
- Aggregate Supply & Demand
- Consumer Prices (CPI)
- Poverty (Poverty Rates)
- Monetary Policies
- Fiscal Policies
- External Debt

In this section we'll take a look at the most relevant. We'll first prepare our document:

We'll import the following libraries:

##### **Code**
```R
library(readr)
library(readxl)
library(writexl)
library(openxlsx)
library(dplyr)
library(tidyr)
library(data.table)
library(stringr)
library(lubridate)
library(car)
library(broom)
library(ggplot2)
library(ggalt)
library(RColorBrewer)
library(extrafont)
library(viridis)
```

Next, we'll define some variables & parameters:

```R
# Directories
rDir <- "data/raw/"
w_FigDir <- "outputs/figures"
utilDir <- file.path(rDir, "Utilities")

# Plotting
color_black <- "#1a1a1a"
color_white <- "#f2f2f2"
color_gray <- "gray"

# Set up scheme as Viridis, with n colors
color_scheme <- viridis::viridis(3)

# Set up visualizations theme
theme_set(theme_gray(base_size = 14) +
  theme(
    text = element_text(family = "Work Sans"),
    axis.text = element_text(color = color_black),
    plot.title = element_text(face = "plain", hjust = 0.5), # Reduced boldness of the title
    panel.background = element_rect(fill = "gray90"),
    panel.grid.major = element_line(color = "white"),
    panel.grid.minor = element_line(color = "white")
  ))
```

Finally, we'll load all our datasets:

```R
# Load data
df_gdp_nominal <- read_csv(file.path(rDir, "GDP_Nominal/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_6011335.csv"), show_col_types = FALSE)
df_gdp_ppp <- read_csv(file.path(rDir, "GDP_PPP/API_NY.GDP.MKTP.PP.CD_DS2_en_csv_v2_5996066.csv"), show_col_types = FALSE)
df_gdp_pc <- read_csv(file.path(rDir, "GDP_Per_Capita/API_NY.GDP.PCAP.PP.CD_DS2_en_csv_v2_6011310.csv"), show_col_types = FALSE)
df_gdp_gr <- read_csv(file.path(rDir, "GDP_Growth/API_NY.GDP.MKTP.KD.ZG_DS2_en_csv_v2_5994650.csv"), show_col_types = FALSE)
df_gdp_pc_gr <- read_csv(file.path(rDir, "GDP_Per_Capita_Growth/API_NY.GDP.PCAP.KD.ZG_DS2_en_csv_v2_5994795.csv"), show_col_types = FALSE)
```

We can now start discussing each indicator.

## 1. GDP
Gross Domestic Product (GDP) has been one of the key metrics used by economists & policy makers throughout the years. So much so that a period of recession is marked by changes in GDP. Roughly speaking, it measures the productive output of a given economy under a given span of time, usually annually. Of course, GDP does not tell the whole story of an economy's performance, and economists figured this out long ago. Nonetheless, GDP is still widely used as a main marker for probing an economy.

### 1.1 Nominal GDP 
- **Description**: The Nominal Gross Domestic Product (*Nominal GDP*) is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars. Dollar figures for GDP are converted from domestic currencies using single year official exchange rates.
- **Source**: [The World Bank](https://data.worldbank.org/indicator/NY.GDP.MKTP.CD)
- **Indicator Code**: NY.GDP.MKTP.CD

We can start by taking a look at the 10 top & bottom countries in terms of Nominal GDP in 2022:




### 1.2 GDP (PPP)


### 1.3 GDP per Capita (PPP)


### 1.4 GDP Growth Rate


### 1.5 GDP per Capita Growth Rate

### 1.6 GNI Star (GNI*)


### Exploring the outliers
- [**Ireland**](https://www.politico.eu/article/ireland-gdp-growth-multinationals-misleading/)
	- Even with PPP adjustment, Ireland's GPD has consistently been in the 3 highest spots throughout the last decade, not only in Europe, but in the world. The problem with this approach is that, as we already mentioned, GDP does not strictly account for the money that goes out of the country to other economies. GDP calculates the monetary value of all finished goods and services made within a country during a specific period, and while this monetary value can very well be produced from within an economy, it can go out to other economies as well, and this is not accounted for in GDP.
	- Turns out that there are 1,500+ multinational companies operating in Ireland because of the lax corporate taxation scheme. This is similar to what happens in Luxembourg, which we'll review in a second.
	- Moreover, a big fraction of these companies are actually healthcare companies. It makes sense then that Ireland was one of the counted economies that had a positive change in GDP per Capita from 2019 to 2020 (*5.10% increase*).
	- This can be due to:
		- Shift of HQs
		- Shift of capital assets such as technology patents & aircraft fleets.
	- The problems:
		- This provides a misleading picture.
		- Credit ratings agencies assess risk of default, in part, by citing a country’s debt-to-GDP ratio; the lower the ratio, the lower the interest rates, and the higher the capacity for an economy to refinance much of its national debt at record-low rates.
	- Let us explore more in detail which are the main companies by sector.
- [**Guyana**](https://www.reuters.com/world/americas/oil-rich-guyana-expects-annual-economic-growth-over-25-coming-years-2023-02-16/)
	- Guyana has been the subject of international news this year due to Venezuela's President Nicolás Maduro putting increased pressure on reclaiming Essequibo, [a region discovered to have massive oil deposits in 2015](https://www.bbc.com/news/world-latin-america-67610200).
	- It has also been subject of international attention due to its unusual growth in GDP per Capita. This is directly linked to oil deposit discovery, and particularly in 2020 to the [start-up of its oil export industry](https://repositorio.cepal.org/server/api/core/bitstreams/7bfd0d40-230f-445a-a61b-8318a7ecdc19/content).
- [Libya](https://www.bmz.de/en/countries/libya/economic-situation-152402)
	- Libya is also a particular case due to its rapid but volatile GDP per Capita Growth Rate in recent years. If we look at the average growth over the last 10 years, we see around 6.25%, but when we take a look at the standard deviation of these percentual changes (*around 36%*), the history is more interesting. Libya has two main highlights:
		- **Libyan civil war (2014–2020)**: This caused heavy fluctuations in the production capacity...
		- **Almost total dependence on oil & gas**: Libyan economy is strongly made up of oil & gas industry; hydrocarbon production currently makes up around 95% of exports and government revenue, and [about 60% of GDP](https://www.bmz.de/en/countries/libya/economic-situation-152402). This means that a generous part of the monetized productive output of the country is based on oil prices; if prices go up, oil-centric countries perceive profits much closer than other countries with more widely-spread productive activities. On contrast, if prices go down, the economy becomes extremely vulnerable. This is very clear if we see that on 2020, the GDP per Capita Growth Rate was around -30.1%. This matches with the [initial & agressive plummeting of oil prices due to the COVID pandemic](https://www.investopedia.com/articles/investing/100615/will-oil-prices-go-2017.asp). Next, 2021 arrives & a surge in oil demand accompanied by a short of supply due to the Russian-Ukrainian war. This creates a strong increase in GDP PC Growth Rate (*around 29.80%*) and is even more pronounced due to 2020's drastically low numbers.


	- If we base our conclusions regarding economic growth & prosperity on one single indicator (*such as GDP or GDP PC*), we risk   

---

# Conclusions
Aaa

---

# References
- https://junkcharts.typepad.com/numbersruleyourworld/
- https://junkcharts.typepad.com/
- TAKE ALL WEBSITES FROM HYPERLINKS IN ARTICLE AND PUT THEM HERE
- https://ggplot2.tidyverse.org/reference/
- https://www.britannica.com/money/topic/macroeconomics
- https://www.imf.org/external/pubs/ft/fandd/2014/09/basics.htm
- https://www.britannica.com/money/topic/monetarism
- https://www.encyclopedia.com/social-sciences/applied-and-social-sciences-magazines/economics-new-classical
- https://corporatefinanceinstitute.com/resources/economics/new-keynesian-economics/
- https://www.britannica.com/money/supply-side-economics
- https://www.politico.eu/article/ireland-gdp-growth-multinationals-misleading/
- https://www.youtube.com/watch?v=rm0BQSWoJlc&list=PLyogaPCPr32W9wbszOANRJiAvUbbymcCS
- https://www.datacamp.com/blog/all-about-r
- https://r4ds.had.co.nz/index.html
- https://baselinesupport.campuslabs.com/hc/en-us/articles/204305665-Types-of-Descriptive-Statistics
- https://mathworld.wolfram.com/MonotonicFunction.html
- https://bachfischer.me/posts/2021/03/analysis_of_swiss_dataset_in_r

---

# Copyright
Pablo Aguirre, Creative Commons Attribution 4.0 International, All Rights Reserved.